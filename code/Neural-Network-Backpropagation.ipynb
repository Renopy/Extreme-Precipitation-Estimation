{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mean_squared_error_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Initialize weights\n",
    "        self.W1 = np.random.randn(input_size, hidden_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.z2  # No activation for the output layer (regression)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y, output, learning_rate):\n",
    "        m = y.shape[0]\n",
    "\n",
    "        # Calculate the loss derivative\n",
    "        loss_derivative = mean_squared_error_derivative(y, output)\n",
    "\n",
    "        # Backpropagation\n",
    "        dW2 = np.dot(self.a1.T, loss_derivative) / m\n",
    "        db2 = np.sum(loss_derivative, axis=0, keepdims=True) / m\n",
    "        dW1 = np.dot(X.T, (np.dot(loss_derivative, self.W2.T) * sigmoid_derivative(self.a1))) / m\n",
    "        db1 = np.sum(np.dot(loss_derivative, self.W2.T) * sigmoid_derivative(self.a1), axis=0, keepdims=True) / m\n",
    "\n",
    "        # Update weights\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output, learning_rate)\n",
    "            if (epoch+1) % 100 == 0:\n",
    "                loss = mean_squared_error(y, output)\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}')\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = NeuralNetwork(input_size=  100 , hidden_size =  8 , output_size = 2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 2.1638\n",
      "Epoch 200/1000, Loss: 1.7674\n",
      "Epoch 300/1000, Loss: 1.4283\n",
      "Epoch 400/1000, Loss: 1.1419\n",
      "Epoch 500/1000, Loss: 0.9041\n",
      "Epoch 600/1000, Loss: 0.7100\n",
      "Epoch 700/1000, Loss: 0.5542\n",
      "Epoch 800/1000, Loss: 0.4311\n",
      "Epoch 900/1000, Loss: 0.3351\n",
      "Epoch 1000/1000, Loss: 0.2610\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 3)  # 1000 samples, 3 features\n",
    "y = X[:, 0] * 2 + X[:, 1] * -3 + X[:, 2] * 5 + np.random.randn(1000) * 0.1  # Target variable with some noise\n",
    "y = y.reshape(-1, 1)  # Reshape y to be a column vector\n",
    "\n",
    "# Initialize the neural network\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the neural network\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "nn.train(X, y, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 320756.6128977641\n",
      "Kernel Gradient Shape: (2, 3, 3, 3, 3)\n",
      "Input Gradient Shape: (3, 10, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def conv3d(input, kernel, stride=1, padding=0):\n",
    "    input_padded = np.pad(input, ((0, 0), (padding, padding), (padding, padding), (padding, padding)), mode='constant', constant_values=0)\n",
    "    C, D, H, W = input.shape\n",
    "    K, _, kD, kH, kW = kernel.shape\n",
    "    out_D = (D - kD + 2 * padding) // stride + 1\n",
    "    out_H = (H - kH + 2 * padding) // stride + 1\n",
    "    out_W = (W - kW + 2 * padding) // stride + 1\n",
    "    output = np.zeros((K, out_D, out_H, out_W))\n",
    "    for k in range(K):\n",
    "        for d in range(out_D):\n",
    "            for h in range(out_H):\n",
    "                for w in range(out_W):\n",
    "                    d_start = d * stride\n",
    "                    h_start = h * stride\n",
    "                    w_start = w * stride\n",
    "                    d_end = d_start + kD\n",
    "                    h_end = h_start + kH\n",
    "                    w_end = w_start + kW\n",
    "                    output[k, d, h, w] = np.sum(input_padded[:, d_start:d_end, h_start:h_end, w_start:w_end] * kernel[k, :, :, :, :])\n",
    "    return output\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def squared_error_loss(output, target):\n",
    "    return 0.5 * np.sum((output - target) ** 2)\n",
    "\n",
    "def squared_error_loss_derivative(output, target):\n",
    "    return output - target\n",
    "\n",
    "def conv3d_backward(input, kernel, output_grad, stride=1, padding=0):\n",
    "    input_padded = np.pad(input, ((0, 0), (padding, padding), (padding, padding), (padding, padding)), mode='constant', constant_values=0)\n",
    "    C, D, H, W = input.shape\n",
    "    K, _, kD, kH, kW = kernel.shape\n",
    "    out_D, out_H, out_W = output_grad.shape[1:]\n",
    "    input_grad = np.zeros_like(input_padded)\n",
    "    kernel_grad = np.zeros_like(kernel)\n",
    "    for k in range(K):\n",
    "        for d in range(out_D):\n",
    "            for h in range(out_H):\n",
    "                for w in range(out_W):\n",
    "                    d_start = d * stride\n",
    "                    h_start = h * stride\n",
    "                    w_start = w * stride\n",
    "                    d_end = d_start + kD\n",
    "                    h_end = h_start + kH\n",
    "                    w_end = w_start + kW\n",
    "                    input_grad[:, d_start:d_end, h_start:h_end, w_start:w_end] += output_grad[k, d, h, w] * kernel[k, :, :, :, :]\n",
    "                    kernel_grad[k, :, :, :, :] += output_grad[k, d, h, w] * input_padded[:, d_start:d_end, h_start:h_end, w_start:w_end]\n",
    "    if padding > 0:\n",
    "        input_grad = input_grad[:, padding:-padding, padding:-padding, padding:-padding]\n",
    "    return input_grad, kernel_grad\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_volume = np.random.rand(3, 10, 10, 10)\n",
    "    kernel = np.random.rand(2, 3, 3, 3, 3)\n",
    "    target = np.random.rand(10)\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    # Forward pass\n",
    "    conv_output = conv3d(input_volume, kernel, stride=1, padding=1)\n",
    "    activated_output = relu(conv_output)\n",
    "    loss = squared_error_loss(activated_output, target)\n",
    "\n",
    "    # Backward pass\n",
    "    loss_grad = squared_error_loss_derivative(activated_output, target)\n",
    "    relu_grad = relu_derivative(conv_output) * loss_grad\n",
    "    input_grad, kernel_grad = conv3d_backward(input_volume, kernel, relu_grad, stride=1, padding=1)\n",
    "\n",
    "    # Update weights\n",
    "    kernel -= learning_rate * kernel_grad\n",
    "\n",
    "    print(\"Loss:\", loss)\n",
    "    print(\"Kernel Gradient Shape:\", kernel_grad.shape)\n",
    "    print(\"Input Gradient Shape:\", input_grad.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 3802610578.805309\n",
      "Epoch 2/10, Loss: 379306239693.6465\n",
      "Epoch 3/10, Loss: 30723805414799.082\n",
      "Epoch 4/10, Loss: 2488628238598339.5\n",
      "Epoch 5/10, Loss: 2.0157888732646506e+17\n",
      "Epoch 6/10, Loss: 1.632788987344367e+19\n",
      "Epoch 7/10, Loss: 1.3225590797489372e+21\n",
      "Epoch 8/10, Loss: 1.071272854596639e+23\n",
      "Epoch 9/10, Loss: 8.677310122232775e+24\n",
      "Epoch 10/10, Loss: 7.02862119900855e+26\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def conv2d(input, kernel, stride=1, padding=0):\n",
    "    batch_size, H, W, C = input.shape\n",
    "    K, kH, kW, _ = kernel.shape\n",
    "    out_H = (H - kH + 2 * padding) // stride + 1\n",
    "    out_W = (W - kW + 2 * padding) // stride + 1\n",
    "    output = np.zeros((batch_size, out_H, out_W, K))\n",
    "    input_padded = np.pad(input, ((0, 0), (padding, padding), (padding, padding), (0, 0)), mode='constant', constant_values=0)\n",
    "    for b in range(batch_size):\n",
    "        for k in range(K):\n",
    "            for h in range(out_H):\n",
    "                for w in range(out_W):\n",
    "                    h_start = h * stride\n",
    "                    w_start = w * stride\n",
    "                    h_end = h_start + kH\n",
    "                    w_end = w_start + kW\n",
    "                    output[b, h, w, k] = np.sum(input_padded[b, h_start:h_end, w_start:w_end, :] * kernel[k, :, :, :])\n",
    "    return output\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def squared_error_loss(output, target):\n",
    "    return 0.5 * np.sum((output - target) ** 2)\n",
    "\n",
    "def squared_error_loss_derivative(output, target):\n",
    "    return output - target\n",
    "\n",
    "def conv2d_backward(input, kernel, output_grad, stride=1, padding=0):\n",
    "    batch_size, H, W, C = input.shape\n",
    "    K, kH, kW, _ = kernel.shape\n",
    "    out_H, out_W = output_grad.shape[1:3]\n",
    "    input_grad = np.zeros_like(input)\n",
    "    kernel_grad = np.zeros_like(kernel)\n",
    "    input_padded = np.pad(input, ((0, 0), (padding, padding), (padding, padding), (0, 0)), mode='constant', constant_values=0)\n",
    "    input_grad_padded = np.pad(input_grad, ((0, 0), (padding, padding), (padding, padding), (0, 0)), mode='constant', constant_values=0)\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        for k in range(K):\n",
    "            for h in range(out_H):\n",
    "                for w in range(out_W):\n",
    "                    h_start = h * stride\n",
    "                    w_start = w * stride\n",
    "                    h_end = h_start + kH\n",
    "                    w_end = w_start + kW\n",
    "                    input_grad_padded[b, h_start:h_end, w_start:w_end, :] += output_grad[b, h, w, k] * kernel[k, :, :, :]\n",
    "                    kernel_grad[k, :, :, :] += output_grad[b, h, w, k] * input_padded[b, h_start:h_end, w_start:w_end, :]\n",
    "    \n",
    "    if padding > 0:\n",
    "        input_grad = input_grad_padded[:, padding:-padding, padding:-padding, :]\n",
    "    else:\n",
    "        input_grad = input_grad_padded\n",
    "    \n",
    "    return input_grad, kernel_grad\n",
    "\n",
    "def flatten(input):\n",
    "    batch_size = input.shape[0]\n",
    "    return input.reshape(batch_size, -1)\n",
    "\n",
    "def dense(input, weights, biases):\n",
    "    return np.dot(input, weights) + biases\n",
    "\n",
    "def dense_backward(input, weights, output_grad):\n",
    "    input_grad = np.dot(output_grad, weights.T)\n",
    "    weights_grad = np.dot(input.T, output_grad)\n",
    "    biases_grad = np.sum(output_grad, axis=0)\n",
    "    return input_grad, weights_grad, biases_grad\n",
    "\n",
    "def train(input_volume, target, kernel, dense_weights, dense_biases, learning_rate=0.01, epochs=10, stride=1, padding=1):\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        conv_output = conv2d(input_volume, kernel, stride=stride, padding=padding)\n",
    "        activated_output = relu(conv_output)\n",
    "        flattened_output = flatten(activated_output)\n",
    "        dense_output = dense(flattened_output, dense_weights, dense_biases)\n",
    "        loss = squared_error_loss(dense_output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss_grad = squared_error_loss_derivative(dense_output, target)\n",
    "        dense_input_grad, dense_weights_grad, dense_biases_grad = dense_backward(flattened_output, dense_weights, loss_grad)\n",
    "        relu_grad = relu_derivative(activated_output) * dense_input_grad.reshape(activated_output.shape)\n",
    "        input_grad, kernel_grad = conv2d_backward(input_volume, kernel, relu_grad, stride=stride, padding=padding)\n",
    "        \n",
    "        # Update weights\n",
    "        kernel -= learning_rate * kernel_grad\n",
    "        dense_weights -= learning_rate * dense_weights_grad\n",
    "        dense_biases -= learning_rate * dense_biases_grad\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    n_samples = 100\n",
    "    h = 7\n",
    "    w = 7\n",
    "    s = 3\n",
    "    l = 64\n",
    "    output_size = 1  # Assuming a single output per sample\n",
    "    input_volume = np.random.rand(n_samples, h, w, s)  # Batch of 5 samples, each of shape (h, w, s)\n",
    "    kernel = np.random.rand(l, 3, 3, s)  # 2 output channels, kernel size 3x3, s input channels\n",
    "    target = np.random.rand(n_samples, output_size)  # Batch of 5 target samples, each of shape (output_size)\n",
    "    dense_weights = np.random.rand(h * w * l, output_size)  # Fully connected layer weights\n",
    "    dense_biases = np.random.rand(output_size)  # Fully connected layer biases\n",
    "    learning_rate = 0.1\n",
    "    epochs = 30\n",
    "\n",
    "    train(input_volume, target, kernel, dense_weights, dense_biases, learning_rate=learning_rate , epochs=10, stride=1, padding=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "name 'learning_rate' is parameter and global (3563962624.py, line 123)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[15], line 123\u001b[1;36m\u001b[0m\n\u001b[1;33m    global learning_rate\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m name 'learning_rate' is parameter and global\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Conv2DLayer:\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, stride=1, padding=0):\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.kernel = np.random.rand(output_channels, kernel_size, kernel_size, input_channels)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        batch_size, H, W, C = input.shape\n",
    "        K, kH, kW, _ = self.kernel.shape\n",
    "        out_H = (H - kH + 2 * self.padding) // self.stride + 1\n",
    "        out_W = (W - kW + 2 * self.padding) // self.stride + 1\n",
    "        output = np.zeros((batch_size, out_H, out_W, K))\n",
    "        input_padded = np.pad(input, ((0, 0), (self.padding, self.padding), (self.padding, self.padding), (0, 0)), mode='constant', constant_values=0)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for k in range(K):\n",
    "                for h in range(out_H):\n",
    "                    for w in range(out_W):\n",
    "                        h_start = h * self.stride\n",
    "                        w_start = w * self.stride\n",
    "                        h_end = h_start + kH\n",
    "                        w_end = w_start + kW\n",
    "                        output[b, h, w, k] = np.sum(input_padded[b, h_start:h_end, w_start:w_end, :] * self.kernel[k, :, :, :])\n",
    "        return output\n",
    "\n",
    "    def backward(self, input, output_grad):\n",
    "        batch_size, H, W, C = input.shape\n",
    "        K, kH, kW, _ = self.kernel.shape\n",
    "        out_H, out_W = output_grad.shape[1:3]\n",
    "        input_grad = np.zeros_like(input)\n",
    "        kernel_grad = np.zeros_like(self.kernel)\n",
    "        input_padded = np.pad(input, ((0, 0), (self.padding, self.padding), (self.padding, self.padding), (0, 0)), mode='constant', constant_values=0)\n",
    "        input_grad_padded = np.pad(input_grad, ((0, 0), (self.padding, self.padding), (self.padding, self.padding), (0, 0)), mode='constant', constant_values=0)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for k in range(K):\n",
    "                for h in range(out_H):\n",
    "                    for w in range(out_W):\n",
    "                        h_start = h * self.stride\n",
    "                        w_start = w * self.stride\n",
    "                        h_end = h_start + kH\n",
    "                        w_end = w_start + kW\n",
    "                        input_grad_padded[b, h_start:h_end, w_start:w_end, :] += output_grad[b, h, w, k] * self.kernel[k, :, :, :]\n",
    "                        kernel_grad[k, :, :, :] += output_grad[b, h, w, k] * input_padded[b, h_start:h_end, w_start:w_end, :]\n",
    "        \n",
    "        if self.padding > 0:\n",
    "            input_grad = input_grad_padded[:, self.padding:-self.padding, self.padding:-self.padding, :]\n",
    "        else:\n",
    "            input_grad = input_grad_padded\n",
    "        \n",
    "        return input_grad, kernel_grad\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size)\n",
    "        self.biases = np.random.rand(output_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return np.dot(input, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, input, output_grad):\n",
    "        input_grad = np.dot(output_grad, self.weights.T)\n",
    "        weights_grad = np.dot(input.T, output_grad)\n",
    "        biases_grad = np.sum(output_grad, axis=0)\n",
    "        return input_grad, weights_grad, biases_grad\n",
    "\n",
    "class ConvNet:\n",
    "    def __init__(self):\n",
    "        self.conv_layers = []\n",
    "        self.dense_layer = None\n",
    "    \n",
    "    def add_conv_layer(self, input_channels, output_channels, kernel_size, stride=1, padding=0):\n",
    "        self.conv_layers.append(Conv2DLayer(input_channels, output_channels, kernel_size, stride, padding))\n",
    "    \n",
    "    def add_dense_layer(self, input_size, output_size):\n",
    "        self.dense_layer = DenseLayer(input_size, output_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        for layer in self.conv_layers:\n",
    "            x = relu(layer.forward(x))\n",
    "        x = flatten(x)\n",
    "        x = self.dense_layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        # Forward pass\n",
    "        x = input\n",
    "        activations = [x]\n",
    "        for layer in self.conv_layers:\n",
    "            x = relu(layer.forward(x))\n",
    "            activations.append(x)\n",
    "        x = flatten(x)\n",
    "        dense_input = x\n",
    "        dense_output = self.dense_layer.forward(x)\n",
    "        loss = squared_error_loss(dense_output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss_grad = squared_error_loss_derivative(dense_output, target)\n",
    "        dense_input_grad, dense_weights_grad, dense_biases_grad = self.dense_layer.backward(dense_input, loss_grad)\n",
    "        \n",
    "        # Reshape dense_input_grad to match the shape of the last conv layer output\n",
    "        dense_input_grad = dense_input_grad.reshape(activations[-1].shape)\n",
    "        \n",
    "        next_grad = dense_input_grad\n",
    "        for i in reversed(range(len(self.conv_layers))):\n",
    "            relu_grad = relu_derivative(activations[i+1]) * next_grad\n",
    "            input_grad, kernel_grad = self.conv_layers[i].backward(activations[i], relu_grad)\n",
    "            next_grad = input_grad\n",
    "            self.conv_layers[i].kernel -= learning_rate * kernel_grad\n",
    "        \n",
    "        self.dense_layer.weights -= learning_rate * dense_weights_grad\n",
    "        self.dense_layer.biases -= learning_rate * dense_biases_grad\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def train(self, input_volume, target, learning_rate=0.01, epochs=10):\n",
    "        global learning_rate \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randn(28, 28)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
