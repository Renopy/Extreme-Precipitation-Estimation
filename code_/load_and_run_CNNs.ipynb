{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import netCDF4\n",
    "import pickle \n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py \n",
    "import numpy as np\n",
    "\n",
    "def Reshape( X ):\n",
    "    x =[ ]\n",
    "    for i in range(X.shape[1]):\n",
    "        x.append(X[: , i  , :])\n",
    "    return np.array(x)\n",
    "\n",
    "def rescale( data  ,Ref ) :\n",
    "    means = np.mean(Ref , axis = 0 )\n",
    "    std = np.std( Ref , axis =0)\n",
    "    return (data -means ) / std\n",
    "\n",
    "def Dict_to_hdf(Dict , of ):\n",
    "    HDF = h5py.File(of, 'w')\n",
    "    for k in Dict :    \n",
    "        test = HDF.create_dataset( k , data = Dict[k] )\n",
    "    HDF.close()   \n",
    "\n",
    "\n",
    "def nc_to_pd(nc):\n",
    "    try:\n",
    "        Keys = list(nc.keys())\n",
    "    except:\n",
    "        Keys = list(nc.variables)\n",
    "    Panda ={ }\n",
    "    for k in Keys:\n",
    "        Panda[k]= np.array(nc[k] )    \n",
    "    return pd.DataFrame(Panda)\n",
    "\n",
    "def DF_bites_to_str(DF):\n",
    "        \n",
    "    for k in DF :\n",
    "        if type(DF[k][0])  == np.bytes_ :\n",
    "            DF[k]  = np.array( DF[k]).astype(str)\n",
    "    return DF\n",
    "\n",
    "def Decode( List, Format ):\n",
    "    a = [ ]\n",
    "    for l in List:\n",
    "        a.append(   l.decode(Format))\n",
    "    return np.array(a)\n",
    "def Sum(L1 , L2):\n",
    "    L = []\n",
    "    for l in range(len(L1)) :\n",
    "        L.append(L1[l] + L2[l]) \n",
    "    return L\n",
    "\n",
    "def Split(DF):\n",
    "    df_train = DF[DF['YEAR']  <= 2020  ]\n",
    "    df_test = DF[DF['YEAR']  > 2020]\n",
    "\n",
    "    df = {}\n",
    "\n",
    "    df['train']  , df['test']  =  df_train , df_test\n",
    "\n",
    "\n",
    "    X = {}\n",
    "\n",
    "    X_train =[]\n",
    "    X_test =[]\n",
    "\n",
    "    X_DF = []\n",
    "\n",
    "    for K in ord_Keys : \n",
    "\n",
    "        X['train'] = df['train'][ K  ].values\n",
    "        X['test'] = df['test'][ K  ].values\n",
    "        X_train.append(rescale(  X['train'] , X['train']  )) \n",
    "        X_test.append( rescale(  X['test'] , X['train']  ) )\n",
    "\n",
    "        X_DF.append(rescale(   DF[ K  ].values , X['train']  ))\n",
    "\n",
    "    obs_train  = np.array(df['train'][obs_key])\n",
    "    obs_test = np.array(df['test'][obs_key])\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    X_DF  = np.array(X_DF)\n",
    "\n",
    "    return  X_train , X_test , X_DF  , obs_train , obs_test\n",
    "\n",
    "\n",
    "def Split_for_image(DF , df_image ):\n",
    "    df_train = DF[DF['YEAR']  <= 2020  ]\n",
    "    df_test = df_image \n",
    "\n",
    "    df = {}\n",
    "\n",
    "    df['train']  , df['test']  =  df_train , df_test\n",
    "\n",
    "\n",
    "    X = {}\n",
    "\n",
    "    X_train =[]\n",
    "    X_test =[]\n",
    "\n",
    "    X_DF = []\n",
    "\n",
    "    for K in ord_Keys : \n",
    "\n",
    "        X['train'] = df['train'][ K  ].values\n",
    "        X['test'] = df['test'][ K  ].values\n",
    "        X_train.append(rescale(  X['train'] , X['train']  )) \n",
    "        X_test.append( rescale(  X['test'] , X['train']  ) )\n",
    "\n",
    "        X_DF.append(rescale(   DF[ K  ].values , X['train']  ))\n",
    "\n",
    "    X_train = np.array(X_train).astype(float)\n",
    "    X_image = np.array(X_test).astype(float)\n",
    "    X_DF  = np.array(X_DF).astype(float)\n",
    "\n",
    "    return  X_train , X_image    \n",
    "\n",
    "def Read(DF ,  hdfs , ord_Keys ) : \n",
    "    \n",
    "    for Keys in ord_Keys:\n",
    "        for k in Keys:\n",
    "            for hdf in hdfs :\n",
    "                if k in list(hdf.keys()):\n",
    "                    DF[k] = np.array(hdf[k])\n",
    "    return DF\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dataframe_multi_dim :\n",
    "\n",
    "    def __init__(self  , Dict ):\n",
    "        self.df  = Dict\n",
    "        self.Keys = list(self.df.keys())\n",
    "        \n",
    "    def Querry(  self , Key  ,  condition  , value) : \n",
    "        \n",
    "        self.df_k = pd.DataFrame(self.df[Key] , columns= [Key])\n",
    "\n",
    "        if condition  == '>=' : \n",
    "            querry = self.df_k[self.df_k[Key]  >= value ] \n",
    "        if condition  == '>' : \n",
    "            querry = self.df_k[self.df_k[Key]  > value ] \n",
    "        if condition  == '==' : \n",
    "            querry = self.df_k[self.df_k[Key]  == value ] \n",
    "        if condition  == '<' : \n",
    "            querry = self.df_k[self.df_k[Key]  < value ] \n",
    "        if condition  == '<=' :\n",
    "            querry = self.df_k[self.df_k[Key]  <= value ] \n",
    "        if condition  == '!=' :\n",
    "            querry = self.df_k[self.df_k[Key]  != value ] \n",
    "        self.Index = np.array(querry.index)\n",
    "        self.out_df  ={ }\n",
    "        \n",
    "        for k in self.Keys  :\n",
    "            self.out_df[k]  = np.array(self.df[k])[self.Index]\n",
    "        self.df  = self.out_df\n",
    "        self.Keys = list(self.df.keys())\n",
    "        return Dataframe_multi_dim(self.df )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['CAL_P', 'CAL_P(-1)', 'CAL_P(-2)', 'CAL_P(-3)', 'CAL_P(-4)', 'CAL_P(-5)', 'CAL_P_ave', 'DAY', 'DEWP', 'DEWP_ATTRIBUTES', 'Date', 'ELEVATION', 'ERA5_cape', 'ERA5_cape(-1)', 'ERA5_cape(-2)', 'ERA5_cape(-3)', 'ERA5_cape(-4)', 'ERA5_cape(-5)', 'ERA5_msl', 'ERA5_msl(-1)', 'ERA5_msl(-2)', 'ERA5_msl(-3)', 'ERA5_msl(-4)', 'ERA5_msl(-5)', 'ERA5_t2m', 'ERA5_t2m(-1)', 'ERA5_t2m(-2)', 'ERA5_t2m(-3)', 'ERA5_t2m(-4)', 'ERA5_t2m(-5)', 'ERA5_tcw', 'ERA5_tcw(-1)', 'ERA5_tcw(-2)', 'ERA5_tcw(-3)', 'ERA5_tcw(-4)', 'ERA5_tcw(-5)', 'ERA5_tp', 'ERA5_tp(-1)', 'ERA5_tp(-2)', 'ERA5_tp(-3)', 'ERA5_tp(-4)', 'ERA5_tp(-5)', 'FRSHTT', 'GUST', 'HQ_P', 'HQ_P(-1)', 'HQ_P(-2)', 'HQ_P(-3)', 'HQ_P(-4)', 'HQ_P(-5)', 'LATITUDE', 'LONGITUDE', 'LSTM_RA_0', 'LSTM_RA_0(-1)', 'LSTM_RA_0(-2)', 'LSTM_RA_0(-3)', 'LSTM_RA_0(-4)', 'LSTM_RA_1', 'LSTM_RA_2', 'LSTM_RA_3', 'LSTM_RA_3(-1)', 'LSTM_RA_3(-2)', 'LSTM_RA_3(-3)', 'LSTM_RA_3(-4)', 'LSTM_RA_4', 'LSTM_RA_4(-4)', 'MAX', 'MAX_ATTRIBUTES', 'MIN', 'MIN_ATTRIBUTES', 'MONTH', 'MXSPD', 'NAME', 'PDIR', 'PDIR(-1)', 'PDIR(-2)', 'PDIR(-3)', 'PDIR(-4)', 'PDIR(-5)', 'PDIR(0)', 'PRCP', 'PRCP_ATTRIBUTES', 'SLP', 'SLP_ATTRIBUTES', 'SNDP', 'STATION', 'STP', 'STP_ATTRIBUTES', 'TEMP', 'TEMP_ATTRIBUTES', 'Unnamed: 0.1', 'VISIB', 'VISIB_ATTRIBUTES', 'WDSP', 'WDSP_ATTRIBUTES', 'YEAR', '_nc4_non_coord_string1', 'dim_0']>\n",
      "<KeysViewHDF5 ['CAL_P', 'DAY', 'LATITUDE', 'LONGITUDE', 'MONTH', 'NAME', 'PRCP', 'STATION', 'YEAR', 'state_code', 'state_name']>\n",
      "<KeysViewHDF5 ['ACO-MLP']>\n",
      "<KeysViewHDF5 ['CNN3', 'CNN3C', 'CNN5', 'CNN5C', 'CNN7', 'CNN7C']>\n",
      "<KeysViewHDF5 ['ACO-MLP', 'DAY', 'LATITUDE', 'LONGITUDE', 'MONTH', 'PRCP', 'YEAR', 'ant_cat', 'state_code']>\n",
      "<KeysViewHDF5 ['CNN7C_re', 'DAY', 'LATITUDE', 'LONGITUDE', 'MONTH', 'PRCP', 'STATE_CODE', 'STATION', 'YEAR']>\n",
      "<KeysViewHDF5 ['ERA5_cape', 'ERA5_cape_Ci_box(3, 3)', 'ERA5_cape_Ci_box(5, 5)', 'ERA5_cape_Ci_box(7, 7)', 'ERA5_cape_Cj_box(3, 3)', 'ERA5_cape_Cj_box(5, 5)', 'ERA5_cape_Cj_box(7, 7)', 'ERA5_cape_ave_box(3, 3)', 'ERA5_cape_ave_box(5, 5)', 'ERA5_cape_ave_box(7, 7)', 'ERA5_cape_std_box(3, 3)', 'ERA5_cape_std_box(5, 5)', 'ERA5_cape_std_box(7, 7)', 'ERA5_t2m', 'ERA5_t2m_Ci_box(3, 3)', 'ERA5_t2m_Ci_box(5, 5)', 'ERA5_t2m_Ci_box(7, 7)', 'ERA5_t2m_Cj_box(3, 3)', 'ERA5_t2m_Cj_box(5, 5)', 'ERA5_t2m_Cj_box(7, 7)', 'ERA5_t2m_ave_box(3, 3)', 'ERA5_t2m_ave_box(5, 5)', 'ERA5_t2m_ave_box(7, 7)', 'ERA5_t2m_std_box(3, 3)', 'ERA5_t2m_std_box(5, 5)', 'ERA5_t2m_std_box(7, 7)', 'ERA5_tcw', 'ERA5_tcw_Ci_box(3, 3)', 'ERA5_tcw_Ci_box(5, 5)', 'ERA5_tcw_Ci_box(7, 7)', 'ERA5_tcw_Cj_box(3, 3)', 'ERA5_tcw_Cj_box(5, 5)', 'ERA5_tcw_Cj_box(7, 7)', 'ERA5_tcw_ave_box(3, 3)', 'ERA5_tcw_ave_box(5, 5)', 'ERA5_tcw_ave_box(7, 7)', 'ERA5_tcw_std_box(3, 3)', 'ERA5_tcw_std_box(5, 5)', 'ERA5_tcw_std_box(7, 7)', 'ERA5_tp', 'ERA5_tp_Ci_box(3, 3)', 'ERA5_tp_Ci_box(5, 5)', 'ERA5_tp_Ci_box(7, 7)', 'ERA5_tp_Cj_box(3, 3)', 'ERA5_tp_Cj_box(5, 5)', 'ERA5_tp_Cj_box(7, 7)', 'ERA5_tp_ave_box(3, 3)', 'ERA5_tp_ave_box(5, 5)', 'ERA5_tp_ave_box(7, 7)', 'ERA5_tp_std_box(3, 3)', 'ERA5_tp_std_box(5, 5)', 'ERA5_tp_std_box(7, 7)']>\n",
      "<KeysViewHDF5 ['cape_3', 'cape_5', 'cape_7', 't2m_3', 't2m_5', 't2m_7', 'tcw_3', 'tcw_5', 'tcw_7', 'tp_3', 'tp_5', 'tp_7']>\n",
      "<KeysViewHDF5 ['CAL_P', 'CAL_P_Ci_box(3, 3)', 'CAL_P_Ci_box(5, 5)', 'CAL_P_Ci_box(7, 7)', 'CAL_P_Cj_box(3, 3)', 'CAL_P_Cj_box(5, 5)', 'CAL_P_Cj_box(7, 7)', 'CAL_P_ave_box(3, 3)', 'CAL_P_ave_box(5, 5)', 'CAL_P_ave_box(7, 7)', 'CAL_P_std_box(3, 3)', 'CAL_P_std_box(5, 5)', 'CAL_P_std_box(7, 7)']>\n",
      "<KeysViewHDF5 ['CAL_P_7']>\n",
      "<KeysViewHDF5 ['CAL_P', 'CAL_P_ave_box(3, 3)', 'CAL_P_ave_box(5, 5)', 'CAL_P_ave_box(7, 7)', 'CAL_P_std_box(3, 3)', 'CAL_P_std_box(5, 5)', 'CAL_P_std_box(7, 7)', 'CNN7', 'DAY', 'ERA5_cape', 'ERA5_cape_ave_box(3, 3)', 'ERA5_cape_ave_box(5, 5)', 'ERA5_cape_ave_box(7, 7)', 'ERA5_cape_std_box(3, 3)', 'ERA5_cape_std_box(5, 5)', 'ERA5_cape_std_box(7, 7)', 'ERA5_t2m', 'ERA5_t2m_ave_box(3, 3)', 'ERA5_t2m_ave_box(5, 5)', 'ERA5_t2m_ave_box(7, 7)', 'ERA5_t2m_std_box(3, 3)', 'ERA5_t2m_std_box(5, 5)', 'ERA5_t2m_std_box(7, 7)', 'ERA5_tcw', 'ERA5_tcw_ave_box(3, 3)', 'ERA5_tcw_ave_box(5, 5)', 'ERA5_tcw_ave_box(7, 7)', 'ERA5_tcw_std_box(3, 3)', 'ERA5_tcw_std_box(5, 5)', 'ERA5_tcw_std_box(7, 7)', 'ERA5_tp', 'ERA5_tp_ave_box(3, 3)', 'ERA5_tp_ave_box(5, 5)', 'ERA5_tp_ave_box(7, 7)', 'ERA5_tp_std_box(3, 3)', 'ERA5_tp_std_box(5, 5)', 'ERA5_tp_std_box(7, 7)', 'LATITUDE', 'LONGITUDE', 'MONTH', 'PDIR', 'PDIR_ave_box(3, 3)', 'PDIR_ave_box(5, 5)', 'PDIR_ave_box(7, 7)', 'PDIR_std_box(3, 3)', 'PDIR_std_box(5, 5)', 'PDIR_std_box(7, 7)', 'PRCP', 'STATE_CODE', 'YEAR']>\n",
      "<KeysViewHDF5 ['PDIR', 'PDIR_Ci_box(3, 3)', 'PDIR_Ci_box(5, 5)', 'PDIR_Ci_box(7, 7)', 'PDIR_Cj_box(3, 3)', 'PDIR_Cj_box(5, 5)', 'PDIR_Cj_box(7, 7)', 'PDIR_ave_box(3, 3)', 'PDIR_ave_box(5, 5)', 'PDIR_ave_box(7, 7)', 'PDIR_std_box(3, 3)', 'PDIR_std_box(5, 5)', 'PDIR_std_box(7, 7)']>\n",
      "<KeysViewHDF5 ['PDIR_3']>\n",
      "<KeysViewHDF5 ['PDIR_5']>\n",
      "<KeysViewHDF5 ['PDIR_7']>\n",
      "<KeysViewHDF5 ['DAY', 'LATITUDE', 'LONGITUDE', 'MONTH', 'NAME', 'STATION', 'YEAR', 're_CAL_P', 're_CAL_P_Ci_box(3, 3)', 're_CAL_P_Ci_box(5, 5)', 're_CAL_P_Ci_box(7, 7)', 're_CAL_P_Cj_box(3, 3)', 're_CAL_P_Cj_box(5, 5)', 're_CAL_P_Cj_box(7, 7)', 're_CAL_P_ave_box(3, 3)', 're_CAL_P_ave_box(5, 5)', 're_CAL_P_ave_box(7, 7)', 're_CAL_P_std_box(3, 3)', 're_CAL_P_std_box(5, 5)', 're_CAL_P_std_box(7, 7)', 're_ERA5_cape', 're_ERA5_cape_Ci_box(3, 3)', 're_ERA5_cape_Ci_box(5, 5)', 're_ERA5_cape_Ci_box(7, 7)', 're_ERA5_cape_Cj_box(3, 3)', 're_ERA5_cape_Cj_box(5, 5)', 're_ERA5_cape_Cj_box(7, 7)', 're_ERA5_cape_ave_box(3, 3)', 're_ERA5_cape_ave_box(5, 5)', 're_ERA5_cape_ave_box(7, 7)', 're_ERA5_cape_std_box(3, 3)', 're_ERA5_cape_std_box(5, 5)', 're_ERA5_cape_std_box(7, 7)', 're_ERA5_t2m', 're_ERA5_t2m_Ci_box(3, 3)', 're_ERA5_t2m_Ci_box(5, 5)', 're_ERA5_t2m_Ci_box(7, 7)', 're_ERA5_t2m_Cj_box(3, 3)', 're_ERA5_t2m_Cj_box(5, 5)', 're_ERA5_t2m_Cj_box(7, 7)', 're_ERA5_t2m_ave_box(3, 3)', 're_ERA5_t2m_ave_box(5, 5)', 're_ERA5_t2m_ave_box(7, 7)', 're_ERA5_t2m_std_box(3, 3)', 're_ERA5_t2m_std_box(5, 5)', 're_ERA5_t2m_std_box(7, 7)', 're_ERA5_tcw', 're_ERA5_tcw_Ci_box(3, 3)', 're_ERA5_tcw_Ci_box(5, 5)', 're_ERA5_tcw_Ci_box(7, 7)', 're_ERA5_tcw_Cj_box(3, 3)', 're_ERA5_tcw_Cj_box(5, 5)', 're_ERA5_tcw_Cj_box(7, 7)', 're_ERA5_tcw_ave_box(3, 3)', 're_ERA5_tcw_ave_box(5, 5)', 're_ERA5_tcw_ave_box(7, 7)', 're_ERA5_tcw_std_box(3, 3)', 're_ERA5_tcw_std_box(5, 5)', 're_ERA5_tcw_std_box(7, 7)', 're_ERA5_tp', 're_ERA5_tp_Ci_box(3, 3)', 're_ERA5_tp_Ci_box(5, 5)', 're_ERA5_tp_Ci_box(7, 7)', 're_ERA5_tp_Cj_box(3, 3)', 're_ERA5_tp_Cj_box(5, 5)', 're_ERA5_tp_Cj_box(7, 7)', 're_ERA5_tp_ave_box(3, 3)', 're_ERA5_tp_ave_box(5, 5)', 're_ERA5_tp_ave_box(7, 7)', 're_ERA5_tp_std_box(3, 3)', 're_ERA5_tp_std_box(5, 5)', 're_ERA5_tp_std_box(7, 7)', 're_PDIR', 're_PDIR_Ci_box(3, 3)', 're_PDIR_Ci_box(5, 5)', 're_PDIR_Ci_box(7, 7)', 're_PDIR_Cj_box(3, 3)', 're_PDIR_Cj_box(5, 5)', 're_PDIR_Cj_box(7, 7)', 're_PDIR_ave_box(3, 3)', 're_PDIR_ave_box(5, 5)', 're_PDIR_ave_box(7, 7)', 're_PDIR_std_box(3, 3)', 're_PDIR_std_box(5, 5)', 're_PDIR_std_box(7, 7)', 're_PRCP']>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Folder = \"D:/Projects/precipitation_AUS/dataset/0025/\"\n",
    "files = os.listdir(Folder)\n",
    "hdfs = []\n",
    "\n",
    "for file in files :\n",
    "    hdf = h5py.File(Folder + file , 'r+')\n",
    "    hdfs.append( hdf)\n",
    "    print(hdf.keys())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Folder = \"D:/Projects/precipitation_AUS/dataset/0025/\"\n",
    "files = os.listdir(Folder)\n",
    "hdfs = []\n",
    "\n",
    "for file in files :\n",
    "    hdf = h5py.File(Folder + file , 'r+')\n",
    "    hdfs.append( hdf)\n",
    "    print(hdf.keys())\n",
    "    \n",
    "needed_keys = [ 'NAME'  , 'LATITUDE' , 'LONGITUDE' ,  'YEAR'  , 'MONTH'   ,  'DAY'  , 'PRCP'  , 'STATE_CODE' , 'PDIR'  , 'ERA5_tp'  , \n",
    "'ERA5_cape' , 'ERA5_tcw'   , 'CAL_P' ] \n",
    "\n",
    "DF = pd.DataFrame()\n",
    "for k in needed_keys :\n",
    "    for hdf in hdfs :\n",
    "        if (k not in list(DF.keys())) and (k in list(hdf.keys())):\n",
    "            DF[k] = np.array(hdf[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDF =  Dataframe_multi_dim(DF)\n",
    "new =  MDF.Querry('YEAR'  ,  '>='  , 2021 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame = new.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NAME': array([b'ADELAIDE INTERNATIONAL', b'ADELAIDE INTERNATIONAL',\n",
       "        b'ADELAIDE INTERNATIONAL', ..., b'DALWALLINU COMPARISON',\n",
       "        b'DALWALLINU COMPARISON', b'DALWALLINU COMPARISON'], dtype=object),\n",
       " 'LATITUDE': array([-34.945    , -34.945    , -34.945    , ..., -30.2833333,\n",
       "        -30.2833333, -30.2833333]),\n",
       " 'LONGITUDE': array([138.530556 , 138.530556 , 138.530556 , ..., 116.6666666,\n",
       "        116.6666666, 116.6666666]),\n",
       " 'YEAR': array([2021, 2021, 2021, ..., 2024, 2024, 2024]),\n",
       " 'MONTH': array([1, 1, 1, ..., 1, 1, 1]),\n",
       " 'DAY': array([ 1,  2,  3, ..., 29, 30, 31]),\n",
       " 'PRCP': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " 'STATE_CODE': array([1, 1, 1, ..., 5, 5, 5], dtype=int64),\n",
       " 'PDIR': array([2., 0., 1., ..., 0., 0., 0.]),\n",
       " 'ERA5_tp': array([1.20376528e-07, 1.12852995e-05, 2.40753056e-07, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'ERA5_cape': array([0.20641509, 1.95449287, 1.83193391, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " 'ERA5_tcw': array([18.76037815, 17.08366836, 19.01739445, ..., 17.44149385,\n",
       "        21.58091224, 16.51304481]),\n",
       " 'CAL_P': array([0.21275145, 0.        , 0.14950539, ..., 0.        , 0.        ,\n",
       "        0.        ], dtype=float32)}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MDF.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf_2 =  Dataframe_multi_dim(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NAME': array([b'ADELAIDE INTERNATIONAL', b'ADELAIDE INTERNATIONAL',\n",
       "        b'ADELAIDE INTERNATIONAL', ..., b'DALWALLINU COMPARISON',\n",
       "        b'DALWALLINU COMPARISON', b'DALWALLINU COMPARISON'], dtype=object),\n",
       " 'LATITUDE': array([-34.945    , -34.945    , -34.945    , ..., -30.2833333,\n",
       "        -30.2833333, -30.2833333]),\n",
       " 'LONGITUDE': array([138.530556 , 138.530556 , 138.530556 , ..., 116.6666666,\n",
       "        116.6666666, 116.6666666]),\n",
       " 'YEAR': array([2021, 2021, 2021, ..., 2024, 2024, 2024]),\n",
       " 'MONTH': array([1, 1, 1, ..., 1, 1, 1]),\n",
       " 'DAY': array([ 1,  2,  3, ..., 29, 30, 31]),\n",
       " 'PRCP': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " 'STATE_CODE': array([1, 1, 1, ..., 5, 5, 5], dtype=int64),\n",
       " 'PDIR': array([2., 0., 1., ..., 0., 0., 0.]),\n",
       " 'ERA5_tp': array([1.20376528e-07, 1.12852995e-05, 2.40753056e-07, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'ERA5_cape': array([0.20641509, 1.95449287, 1.83193391, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " 'ERA5_tcw': array([18.76037815, 17.08366836, 19.01739445, ..., 17.44149385,\n",
       "        21.58091224, 16.51304481]),\n",
       " 'CAL_P': array([0.21275145, 0.        , 0.14950539, ..., 0.        , 0.        ,\n",
       "        0.        ], dtype=float32)}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdf_2.Querry('MONTH'  ,  '=='  , 1 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498184</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498185</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498186</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498187</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498188</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498189 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "...    ..\n",
       "498184  1\n",
       "498185  1\n",
       "498186  1\n",
       "498187  1\n",
       "498188  1\n",
       "\n",
       "[498189 rows x 1 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdf_2.df_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2021, 2021, 2021, ..., 2024, 2024, 2024])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MDF.df['YEAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NAME': array([b'ADELAIDE INTERNATIONAL', b'ADELAIDE INTERNATIONAL',\n",
       "        b'ADELAIDE INTERNATIONAL', ..., b'DALWALLINU COMPARISON',\n",
       "        b'DALWALLINU COMPARISON', b'DALWALLINU COMPARISON'], dtype=object),\n",
       " 'LATITUDE': array([-34.945    , -34.945    , -34.945    , ..., -30.2833333,\n",
       "        -30.2833333, -30.2833333]),\n",
       " 'LONGITUDE': array([138.530556 , 138.530556 , 138.530556 , ..., 116.6666666,\n",
       "        116.6666666, 116.6666666]),\n",
       " 'YEAR': array([2021, 2021, 2021, ..., 2024, 2024, 2024]),\n",
       " 'MONTH': array([1, 1, 1, ..., 1, 1, 1]),\n",
       " 'DAY': array([ 1,  2,  3, ..., 29, 30, 31]),\n",
       " 'PRCP': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " 'STATE_CODE': array([1, 1, 1, ..., 5, 5, 5], dtype=int64),\n",
       " 'PDIR': array([2., 0., 1., ..., 0., 0., 0.]),\n",
       " 'ERA5_tp': array([1.20376528e-07, 1.12852995e-05, 2.40753056e-07, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'ERA5_cape': array([0.20641509, 1.95449287, 1.83193391, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " 'ERA5_tcw': array([18.76037815, 17.08366836, 19.01739445, ..., 17.44149385,\n",
       "        21.58091224, 16.51304481]),\n",
       " 'CAL_P': array([0.21275145, 0.        , 0.14950539, ..., 0.        , 0.        ,\n",
       "        0.        ], dtype=float32)}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmdf.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>STATE_CODE</th>\n",
       "      <th>PDIR</th>\n",
       "      <th>ERA5_tp</th>\n",
       "      <th>ERA5_cape</th>\n",
       "      <th>ERA5_tcw</th>\n",
       "      <th>CAL_P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'ADELAIDE INTERNATIONAL'</td>\n",
       "      <td>-34.945000</td>\n",
       "      <td>138.530556</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.203765e-07</td>\n",
       "      <td>0.206415</td>\n",
       "      <td>18.760378</td>\n",
       "      <td>0.212751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'ADELAIDE INTERNATIONAL'</td>\n",
       "      <td>-34.945000</td>\n",
       "      <td>138.530556</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.128530e-05</td>\n",
       "      <td>1.954493</td>\n",
       "      <td>17.083668</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'ADELAIDE INTERNATIONAL'</td>\n",
       "      <td>-34.945000</td>\n",
       "      <td>138.530556</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.407531e-07</td>\n",
       "      <td>1.831934</td>\n",
       "      <td>19.017394</td>\n",
       "      <td>0.149505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'ADELAIDE INTERNATIONAL'</td>\n",
       "      <td>-34.945000</td>\n",
       "      <td>138.530556</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.784967e-06</td>\n",
       "      <td>1.077229</td>\n",
       "      <td>19.719130</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'ADELAIDE INTERNATIONAL'</td>\n",
       "      <td>-34.945000</td>\n",
       "      <td>138.530556</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.106589e-07</td>\n",
       "      <td>0.245118</td>\n",
       "      <td>14.335306</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498184</th>\n",
       "      <td>b'DALWALLINU COMPARISON'</td>\n",
       "      <td>-30.283333</td>\n",
       "      <td>116.666667</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.975645</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498185</th>\n",
       "      <td>b'DALWALLINU COMPARISON'</td>\n",
       "      <td>-30.283333</td>\n",
       "      <td>116.666667</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.875948</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498186</th>\n",
       "      <td>b'DALWALLINU COMPARISON'</td>\n",
       "      <td>-30.283333</td>\n",
       "      <td>116.666667</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.441494</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498187</th>\n",
       "      <td>b'DALWALLINU COMPARISON'</td>\n",
       "      <td>-30.283333</td>\n",
       "      <td>116.666667</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.580912</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498188</th>\n",
       "      <td>b'DALWALLINU COMPARISON'</td>\n",
       "      <td>-30.283333</td>\n",
       "      <td>116.666667</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.513045</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498189 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             NAME   LATITUDE   LONGITUDE  YEAR  MONTH  DAY  \\\n",
       "0       b'ADELAIDE INTERNATIONAL' -34.945000  138.530556  2021      1    1   \n",
       "1       b'ADELAIDE INTERNATIONAL' -34.945000  138.530556  2021      1    2   \n",
       "2       b'ADELAIDE INTERNATIONAL' -34.945000  138.530556  2021      1    3   \n",
       "3       b'ADELAIDE INTERNATIONAL' -34.945000  138.530556  2021      1    4   \n",
       "4       b'ADELAIDE INTERNATIONAL' -34.945000  138.530556  2021      1    5   \n",
       "...                           ...        ...         ...   ...    ...  ...   \n",
       "498184   b'DALWALLINU COMPARISON' -30.283333  116.666667  2024      1   27   \n",
       "498185   b'DALWALLINU COMPARISON' -30.283333  116.666667  2024      1   28   \n",
       "498186   b'DALWALLINU COMPARISON' -30.283333  116.666667  2024      1   29   \n",
       "498187   b'DALWALLINU COMPARISON' -30.283333  116.666667  2024      1   30   \n",
       "498188   b'DALWALLINU COMPARISON' -30.283333  116.666667  2024      1   31   \n",
       "\n",
       "        PRCP  STATE_CODE  PDIR       ERA5_tp  ERA5_cape   ERA5_tcw     CAL_P  \n",
       "0        0.0           1   2.0  1.203765e-07   0.206415  18.760378  0.212751  \n",
       "1        0.0           1   0.0  1.128530e-05   1.954493  17.083668  0.000000  \n",
       "2        0.0           1   1.0  2.407531e-07   1.831934  19.017394  0.149505  \n",
       "3        0.0           1   0.0  4.784967e-06   1.077229  19.719130  0.000000  \n",
       "4        0.0           1   0.0  2.106589e-07   0.245118  14.335306  0.000000  \n",
       "...      ...         ...   ...           ...        ...        ...       ...  \n",
       "498184   0.0           5   0.0  0.000000e+00   0.000000  13.975645  0.000000  \n",
       "498185   0.0           5   0.0  0.000000e+00   0.000000  17.875948  0.000000  \n",
       "498186   0.0           5   0.0  0.000000e+00   0.000000  17.441494  0.000000  \n",
       "498187   0.0           5   0.0  0.000000e+00   0.000000  21.580912  0.000000  \n",
       "498188   0.0           5   0.0  0.000000e+00   0.000000  16.513045  0.000000  \n",
       "\n",
       "[498189 rows x 13 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(qmdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.707034736754034e-05"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 'ERA5_tp'\n",
    "np.mean(DF[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['DAY', 'LATITUDE', 'LONGITUDE', 'MONTH', 'YEAR']>\n",
      "<KeysViewHDF5 ['ACO-MLP']>\n",
      "<KeysViewHDF5 ['CNN3', 'CNN3C', 'CNN5', 'CNN5C', 'CNN7', 'CNN7C']>\n",
      "<KeysViewHDF5 ['ERA5_cape', 'ERA5_cape_Ci_box(3, 3)', 'ERA5_cape_Ci_box(5, 5)', 'ERA5_cape_Ci_box(7, 7)', 'ERA5_cape_Cj_box(3, 3)', 'ERA5_cape_Cj_box(5, 5)', 'ERA5_cape_Cj_box(7, 7)', 'ERA5_cape_ave_box(3, 3)', 'ERA5_cape_ave_box(5, 5)', 'ERA5_cape_ave_box(7, 7)', 'ERA5_cape_std_box(3, 3)', 'ERA5_cape_std_box(5, 5)', 'ERA5_cape_std_box(7, 7)', 'ERA5_t2m', 'ERA5_t2m_Ci_box(3, 3)', 'ERA5_t2m_Ci_box(5, 5)', 'ERA5_t2m_Ci_box(7, 7)', 'ERA5_t2m_Cj_box(3, 3)', 'ERA5_t2m_Cj_box(5, 5)', 'ERA5_t2m_Cj_box(7, 7)', 'ERA5_t2m_ave_box(3, 3)', 'ERA5_t2m_ave_box(5, 5)', 'ERA5_t2m_ave_box(7, 7)', 'ERA5_t2m_std_box(3, 3)', 'ERA5_t2m_std_box(5, 5)', 'ERA5_t2m_std_box(7, 7)', 'ERA5_tcw', 'ERA5_tcw_Ci_box(3, 3)', 'ERA5_tcw_Ci_box(5, 5)', 'ERA5_tcw_Ci_box(7, 7)', 'ERA5_tcw_Cj_box(3, 3)', 'ERA5_tcw_Cj_box(5, 5)', 'ERA5_tcw_Cj_box(7, 7)', 'ERA5_tcw_ave_box(3, 3)', 'ERA5_tcw_ave_box(5, 5)', 'ERA5_tcw_ave_box(7, 7)', 'ERA5_tcw_std_box(3, 3)', 'ERA5_tcw_std_box(5, 5)', 'ERA5_tcw_std_box(7, 7)', 'ERA5_tp', 'ERA5_tp_Ci_box(3, 3)', 'ERA5_tp_Ci_box(5, 5)', 'ERA5_tp_Ci_box(7, 7)', 'ERA5_tp_Cj_box(3, 3)', 'ERA5_tp_Cj_box(5, 5)', 'ERA5_tp_Cj_box(7, 7)', 'ERA5_tp_ave_box(3, 3)', 'ERA5_tp_ave_box(5, 5)', 'ERA5_tp_ave_box(7, 7)', 'ERA5_tp_std_box(3, 3)', 'ERA5_tp_std_box(5, 5)', 'ERA5_tp_std_box(7, 7)']>\n",
      "<KeysViewHDF5 ['CAL_P', 'CAL_P_Ci_box(3, 3)', 'CAL_P_Ci_box(5, 5)', 'CAL_P_Ci_box(7, 7)', 'CAL_P_Cj_box(3, 3)', 'CAL_P_Cj_box(5, 5)', 'CAL_P_Cj_box(7, 7)', 'CAL_P_ave_box(3, 3)', 'CAL_P_ave_box(5, 5)', 'CAL_P_ave_box(7, 7)', 'CAL_P_std_box(3, 3)', 'CAL_P_std_box(5, 5)', 'CAL_P_std_box(7, 7)']>\n",
      "<KeysViewHDF5 ['PDIR', 'PDIR_Ci_box(3, 3)', 'PDIR_Ci_box(5, 5)', 'PDIR_Ci_box(7, 7)', 'PDIR_Cj_box(3, 3)', 'PDIR_Cj_box(5, 5)', 'PDIR_Cj_box(7, 7)', 'PDIR_ave_box(3, 3)', 'PDIR_ave_box(5, 5)', 'PDIR_ave_box(7, 7)', 'PDIR_std_box(3, 3)', 'PDIR_std_box(5, 5)', 'PDIR_std_box(7, 7)']>\n"
     ]
    }
   ],
   "source": [
    "# hdfs for images\n",
    "Folder = \"D:/Projects/precipitation_AUS/dataset/23feb2022/\"\n",
    "files_images = os.listdir(Folder)\n",
    "hdfs_image = []\n",
    "\n",
    "for file in files_images :\n",
    "    hdf = h5py.File(Folder + file , 'r+')\n",
    "    hdfs_image.append( hdf)\n",
    "    print(hdf.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model\n",
    "cnn_names = ['CNN3'  , 'CNN5'  , 'CNN7'  , 'CNN3C'  , 'CNN5C' ,  'CNN7C' ]\n",
    "out_dict = {}\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "for name in cnn_names : \n",
    "    model = tf.keras.models.load_model( 'D:/Projects/precipitation_AUS/models-2/'+name+ r'.keras' )\n",
    "    model_meta = h5py.File(r\"D:\\Projects\\precipitation_AUS\\models-2/\"+name+r\".hdf5\" )\n",
    "\n",
    "    #model.summary()    \n",
    "\n",
    "    model_name = str(np.array(model_meta['name']))\n",
    "    print(model_name)\n",
    "    ord_Keys = []\n",
    "    binary_keys = np.array(model_meta['ord_keys' ])\n",
    "    for k in binary_keys  : ord_Keys.append( Decode(k  , 'utf-8') ) \n",
    "\n",
    "    DF =  Read( DF , hdfs , ord_Keys )\n",
    "    df_image = Read( pd.DataFrame() , hdfs_image , ord_Keys )\n",
    "    X_train , X_image = Split_for_image(DF , df_image )\n",
    "    x_train =  Reshape(X_train)\n",
    "    x_image  = Reshape(X_image)\n",
    "    \n",
    "    out_dict[model_name] = model.predict( x_image).T[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run models for DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN3\n",
      "\u001b[1m47252/47252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 845us/step\n",
      "CNN5\n",
      "\u001b[1m47252/47252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step\n",
      "CNN7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47252/47252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 2ms/step\n",
      "CNN3C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47252/47252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1ms/step\n",
      "CNN5C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47252/47252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2ms/step\n",
      "CNN7C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n",
      "C:\\Users\\S4055367\\AppData\\Local\\Temp\\ipykernel_14928\\781912636.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  DF[k] = np.array(hdf[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47252/47252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "cnn_names = ['CNN3'  , 'CNN5'  , 'CNN7'  , 'CNN3C'  , 'CNN5C' ,  'CNN7C' ]\n",
    "out_dict = {}\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "obs_key = 'PRCP'\n",
    "for name in cnn_names : \n",
    "    model = tf.keras.models.load_model( 'D:/Projects/precipitation_AUS/models-2/'+name+ r'.keras' )\n",
    "    model_meta = h5py.File(r\"D:\\Projects\\precipitation_AUS\\models-2/\"+name+r\".hdf5\" )\n",
    "\n",
    "    #model.summary()    \n",
    "\n",
    "    model_name = str(np.array(model_meta['name']))\n",
    "    print(model_name[2:-1])\n",
    "    \n",
    "    ord_Keys = []\n",
    "    binary_keys = np.array(model_meta['ord_keys' ])\n",
    "    for k in binary_keys  : ord_Keys.append( Decode(k  , 'utf-8') ) \n",
    "\n",
    "    DF =  Read( DF , hdfs , ord_Keys )\n",
    "    X_train , X_test , X_DF  , obs_train , obs_test = Split(DF)\n",
    "    x = Reshape(X_DF)\n",
    "    out_dict[model_name[2:-1]] = model.predict(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 4.43639234e+00, -5.34347749e-04,  2.74316940e-01,\n",
       "         -2.79031000e-01,  2.08655120e-01,  1.93188567e+00],\n",
       "        [ 3.63487988e+00, -2.87986339e-02,  3.57283312e-01,\n",
       "         -2.79364976e-01,  2.25047113e-01,  1.94004561e+00],\n",
       "        [ 2.22262989e+00,  5.67718156e-01,  1.37948206e+00,\n",
       "         -2.14368613e-01,  1.11486940e+00,  1.29068752e+00]],\n",
       "\n",
       "       [[-2.54941745e-01, -1.25016500e-01,  1.52087096e-01,\n",
       "         -3.18820473e-01, -3.71991168e-01,  1.39133058e+00],\n",
       "        [-2.52625463e-01, -1.54701688e-01,  2.31698007e-01,\n",
       "         -3.21704654e-01, -3.71306233e-01,  1.39537505e+00],\n",
       "        [-2.20789844e-01,  1.32682033e-02,  5.04526628e-01,\n",
       "         -3.18023458e-01,  1.46995911e+00,  3.60582077e-01]],\n",
       "\n",
       "       [[-2.70338066e-01, -2.49498653e-01, -2.41480738e-01,\n",
       "         -2.86600777e-01, -3.93626829e-01, -7.24479024e-01],\n",
       "        [-2.76831614e-01, -2.52626285e-01, -2.52844839e-01,\n",
       "         -2.93414621e-01, -3.71354781e-01, -7.17242794e-01],\n",
       "        [-3.09701535e-01, -3.61745597e-01, -2.65294085e-01,\n",
       "         -3.03362106e-01, -1.76244700e-02, -1.46277248e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.70338066e-01, -2.49498653e-01, -3.02084666e-01,\n",
       "         -3.22272055e-01,  1.09729181e+00, -3.99357542e-01],\n",
       "        [-2.77500310e-01, -2.52626285e-01, -3.16325695e-01,\n",
       "         -3.27709677e-01,  1.11038186e+00, -4.08981204e-01],\n",
       "        [-3.13684173e-01, -3.61745597e-01, -3.53504917e-01,\n",
       "         -3.36202095e-01,  1.07723085e-01, -8.45094775e-02]],\n",
       "\n",
       "       [[-2.70338066e-01, -2.49498653e-01, -3.02084666e-01,\n",
       "         -3.22272055e-01,  1.83389598e+00, -7.20384385e-02],\n",
       "        [-2.77500310e-01, -2.52626285e-01, -3.16325695e-01,\n",
       "         -3.27709677e-01,  1.85528056e+00, -7.49524023e-02],\n",
       "        [-3.13684173e-01, -3.61745597e-01, -3.53504917e-01,\n",
       "         -3.36202095e-01,  4.82086278e-02, -1.42494002e-01]],\n",
       "\n",
       "       [[-2.70338066e-01, -2.49498653e-01, -3.02084666e-01,\n",
       "         -3.22272055e-01,  2.09376016e+00, -4.72773437e-01],\n",
       "        [-2.77500310e-01, -2.52626285e-01, -3.16325695e-01,\n",
       "         -3.27709677e-01,  2.12148285e+00, -4.67716463e-01],\n",
       "        [-3.13684173e-01, -3.61745597e-01, -3.53504917e-01,\n",
       "         -3.36202095e-01, -4.61927639e-01, -1.29547430e-01]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN7C'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name[2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Dict_to_hdf(Dict , of ):\n",
    "    HDF = h5py.File(of, 'w')\n",
    "    for k in Dict :    \n",
    "        test = HDF.create_dataset( k , data = Dict[k] )\n",
    "        print( k)\n",
    "    HDF.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN3\n",
      "CNN5\n",
      "CNN7\n",
      "CNN3C\n",
      "CNN5C\n",
      "CNN7C\n"
     ]
    }
   ],
   "source": [
    "Dict_to_hdf(out_dict , r\"D:\\Projects\\precipitation_AUS\\dataset\\0025\\joint-all-CNNs.hdf5\" ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACOR-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP :\n",
    "    def __init__(self, nn_shape , inp_feat ):\n",
    "        self.nn_shape = nn_shape\n",
    "\n",
    "        self.inp_feat = inp_feat\n",
    "\n",
    "        self.W_Shapes = []\n",
    "        self.B_Shapes = []\n",
    "\n",
    "        for i in range(len(self.nn_shape)):\n",
    "            if i == 0 :\n",
    "                self.W_Shapes .append(  (self.inp_feat,  self.nn_shape[i])    )    \n",
    "            else:\n",
    "                self.W_Shapes .append(  (self.nn_shape[i-1],self.nn_shape[i])    )\n",
    "            self.B_Shapes .append(   self.nn_shape[i] )\n",
    "        self.W_Shapes.append((self.nn_shape[-1] , 1))\n",
    "        self.B_Shapes .append(  1 )\n",
    "        self.w_params =[]\n",
    "\n",
    "        for I in self.W_Shapes:\n",
    "            self.w_params.append(  I[0]*I[1]    )\n",
    "\n",
    "        self.Sum_params = sum(self.w_params ) +sum(self.B_Shapes)\n",
    "        \n",
    "    def update(self, solution):\n",
    "        ii = 0\n",
    "        self.W = []\n",
    "        for i in range(len(self.W_Shapes)) : \n",
    "            a = np.array(solution[ ii  : ii  + self.w_params[i] ])\n",
    "            self.W.append( np.reshape(a, self.W_Shapes[i]))\n",
    "            ii = ii  + self.w_params[i] \n",
    "        self.B= []\n",
    "        for i in range(len(self.B_Shapes)) :\n",
    "            a = np.array(solution[ ii  : ii  + self.B_Shapes[i] ])\n",
    "            self.B.append( a )\n",
    "            ii = ii  +  self.B_Shapes[i]\n",
    "        self.number_of_layers = len(self.W)\n",
    "        \n",
    "    def feed_forward(self , X ):\n",
    "        for i in range(self.number_of_layers):\n",
    "            h = numpy.dot(X,numpy.array(self.W[i])) + self.B[i]\n",
    "            if i!= self.number_of_layers-1:\n",
    "                # Xa = numpy.tanh(h)\n",
    "                X = numpy.maximum(h,0) \n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hdfs_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m CNN7C \u001b[38;5;241m=\u001b[39m  np\u001b[38;5;241m.\u001b[39marray(\u001b[43mhdfs_image\u001b[49m[\u001b[38;5;241m1\u001b[39m][ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNN7C\u001b[39m\u001b[38;5;124m'\u001b[39m] )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hdfs_image' is not defined"
     ]
    }
   ],
   "source": [
    "CNN7C =  np.array(hdfs_image[1][ 'CNN7C'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN7C =  np.array(out_dict[ 'CNN7C'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'../models-2/ACO-MLP.pickle', 'rb') as file2:\n",
    "    model_aco_nn = pickle.load(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[5.23006747],\n",
       "        [5.97031466],\n",
       "        [0.16191503],\n",
       "        ...,\n",
       "        [0.15570654],\n",
       "        [0.16803729],\n",
       "        [0.14672869]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "model_aco_nn.feed_forward( np.array( [CNN7C]  ).T )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict ={}\n",
    "out_dict['ACO-MLP'] = model_aco_nn.feed_forward( np.array( [CNN7C]  ).T ).T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACO-MLP\n"
     ]
    }
   ],
   "source": [
    "Dict_to_hdf(out_dict , r\"D:\\Projects\\precipitation_AUS\\dataset\\0025\\joint-ACO.hdf5\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACO-MLP': array([[5.23006747],\n",
       "        [5.97031466],\n",
       "        [0.16191503],\n",
       "        ...,\n",
       "        [0.15570654],\n",
       "        [0.16803729],\n",
       "        [0.14672869]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF to images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['DAY', 'LATITUDE', 'LONGITUDE', 'MONTH', 'YEAR']>\n",
      "<KeysViewHDF5 ['ACO-MLP']>\n",
      "<KeysViewHDF5 ['CNN3', 'CNN3C', 'CNN5', 'CNN5C', 'CNN7', 'CNN7C']>\n",
      "<KeysViewHDF5 ['ERA5_cape', 'ERA5_cape_Ci_box(3, 3)', 'ERA5_cape_Ci_box(5, 5)', 'ERA5_cape_Ci_box(7, 7)', 'ERA5_cape_Cj_box(3, 3)', 'ERA5_cape_Cj_box(5, 5)', 'ERA5_cape_Cj_box(7, 7)', 'ERA5_cape_ave_box(3, 3)', 'ERA5_cape_ave_box(5, 5)', 'ERA5_cape_ave_box(7, 7)', 'ERA5_cape_std_box(3, 3)', 'ERA5_cape_std_box(5, 5)', 'ERA5_cape_std_box(7, 7)', 'ERA5_t2m', 'ERA5_t2m_Ci_box(3, 3)', 'ERA5_t2m_Ci_box(5, 5)', 'ERA5_t2m_Ci_box(7, 7)', 'ERA5_t2m_Cj_box(3, 3)', 'ERA5_t2m_Cj_box(5, 5)', 'ERA5_t2m_Cj_box(7, 7)', 'ERA5_t2m_ave_box(3, 3)', 'ERA5_t2m_ave_box(5, 5)', 'ERA5_t2m_ave_box(7, 7)', 'ERA5_t2m_std_box(3, 3)', 'ERA5_t2m_std_box(5, 5)', 'ERA5_t2m_std_box(7, 7)', 'ERA5_tcw', 'ERA5_tcw_Ci_box(3, 3)', 'ERA5_tcw_Ci_box(5, 5)', 'ERA5_tcw_Ci_box(7, 7)', 'ERA5_tcw_Cj_box(3, 3)', 'ERA5_tcw_Cj_box(5, 5)', 'ERA5_tcw_Cj_box(7, 7)', 'ERA5_tcw_ave_box(3, 3)', 'ERA5_tcw_ave_box(5, 5)', 'ERA5_tcw_ave_box(7, 7)', 'ERA5_tcw_std_box(3, 3)', 'ERA5_tcw_std_box(5, 5)', 'ERA5_tcw_std_box(7, 7)', 'ERA5_tp', 'ERA5_tp_Ci_box(3, 3)', 'ERA5_tp_Ci_box(5, 5)', 'ERA5_tp_Ci_box(7, 7)', 'ERA5_tp_Cj_box(3, 3)', 'ERA5_tp_Cj_box(5, 5)', 'ERA5_tp_Cj_box(7, 7)', 'ERA5_tp_ave_box(3, 3)', 'ERA5_tp_ave_box(5, 5)', 'ERA5_tp_ave_box(7, 7)', 'ERA5_tp_std_box(3, 3)', 'ERA5_tp_std_box(5, 5)', 'ERA5_tp_std_box(7, 7)']>\n",
      "<KeysViewHDF5 ['CAL_P', 'CAL_P_Ci_box(3, 3)', 'CAL_P_Ci_box(5, 5)', 'CAL_P_Ci_box(7, 7)', 'CAL_P_Cj_box(3, 3)', 'CAL_P_Cj_box(5, 5)', 'CAL_P_Cj_box(7, 7)', 'CAL_P_ave_box(3, 3)', 'CAL_P_ave_box(5, 5)', 'CAL_P_ave_box(7, 7)', 'CAL_P_std_box(3, 3)', 'CAL_P_std_box(5, 5)', 'CAL_P_std_box(7, 7)']>\n",
      "<KeysViewHDF5 ['PDIR', 'PDIR_Ci_box(3, 3)', 'PDIR_Ci_box(5, 5)', 'PDIR_Ci_box(7, 7)', 'PDIR_Cj_box(3, 3)', 'PDIR_Cj_box(5, 5)', 'PDIR_Cj_box(7, 7)', 'PDIR_ave_box(3, 3)', 'PDIR_ave_box(5, 5)', 'PDIR_ave_box(7, 7)', 'PDIR_std_box(3, 3)', 'PDIR_std_box(5, 5)', 'PDIR_std_box(7, 7)']>\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import h5py\n",
    "import tifffile as tff \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# hdfs for images\n",
    "Folder = \"D:/Projects/precipitation_AUS/dataset/23feb2022/\"\n",
    "files_images = os.listdir(Folder)\n",
    "hdfs_image = []\n",
    "\n",
    "for file in files_images :\n",
    "    hdf = h5py.File(Folder + file , 'r+')\n",
    "    print(hdf.keys())\n",
    "    hdfs_image.append( hdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names =  [  'CAL_P' ]\n",
    "\n",
    "import tifffile as tff\n",
    "df_image  = pd.DataFrame() \n",
    "for k in names:\n",
    "    for hdf in hdfs_image :\n",
    "        if k in list(hdf.keys()):\n",
    "            df_image[k] = np.array(hdf[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(313, 211)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_shape = (313, 211)\n",
    "folder  = r'D:\\Projects\\precipitation_AUS\\IMAGES\\run-2/'\n",
    "for name in  names :\n",
    "    image = np.reshape( df_image[name].values , new_shape)\n",
    "    image  = np.maximum( image , 0  )\n",
    "    if name == 'ERA5_tp'   : image = image *1000 *24 \n",
    "\n",
    "    print(image.shape)\n",
    "    of  = folder + name + '.tif'\n",
    "    tff.imwrite(of , image )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.absolute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=0, step=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_image.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
